# -*- coding: utf-8 -*-
"""CSE351-HW2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YQTFGFrGzTPK7edRL7IPLP2g9Vo3qRUf
"""

## do all imports here
import numpy as np
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression

from google.colab import files

## downloaded csv file from url and importing it into notebook
weather_data = files.upload()
energy_data = files.upload()

weather_data = pd.read_csv('weather_data.csv') ## opening weather data
energy_data = pd.read_csv('energy_data.csv') ## opening energy data

## Task 1

copy_energy = energy_data

## adding a column just for the date in copy_energy
just_time = []

for i in range(365):  
  for j in range(48):
    index = i * 48 + j
    time = energy_data.at[index, 'Date & Time'] 
    time = time[0:10]
    just_time.append(time)

copy_energy["Date"] = just_time 


## grouping copy_energy by the date
columns = copy_energy.columns[1:18:1]
daily_energy = copy_energy.groupby(["Date"])[columns].sum() ## adding per day usage

## copying time value in weather data into energy data
energy_every_two = energy_data
energy_every_two = energy_every_two.iloc[::2, :] ## only retreiving every two rows

energy_every_two.head()

unix_times = [] ## converting unix times

## adding unix time to energy_every_two
for i in range(365):  
  for j in range(24):
    index = i * 24 + j
    time = weather_data.at[index, 'time']
    unix_times.append(time)

energy_every_two["time"] = unix_times


merged_data = pd.merge(weather_data, energy_every_two) ## merging the two altered datasets together

## grouping by day in weather_data

columns_to_average = []

## only getting the necesary columns
grouped_merged_data = merged_data.copy()
grouped_merged_data = grouped_merged_data.groupby("Date")["temperature","humidity", "visibility", "pressure", "windSpeed", "cloudCover", "windBearing", "precipIntensity", "dewPoint", "precipProbability"].mean()

## grouped_merged_data.head()
## daily_energy.head()

daily_merged_data = grouped_merged_data.join(daily_energy)

print(daily_merged_data)


## merged_data2 = pd.merge(grouped_merged_data, daily_energy)

## What we need: merge the grouped_energy data with the weather_data that is averaged per day
## print(merged_data2)

## Task 2

## split the merged_data into training and testing datasets
merged_data_copy = merged_data.copy()
daily_merged_data_copy = daily_merged_data.copy()

columns = energy_data.columns[2:18:1]

for i in range(len(columns)):
    del merged_data_copy[columns[i]] ## removing each column
    del daily_merged_data_copy[columns[i]]

training_data = daily_merged_data_copy.head(334)
testing_data = daily_merged_data_copy.tail(31)

testing_data_without_label = testing_data.copy()
y_actual = testing_data["use [kW]"]
del testing_data_without_label["use [kW]"]

## Task 3
from sklearn.linear_model import LinearRegression
import math
from sklearn.metrics import mean_squared_error
from math import sqrt
import csv


columns = testing_data_without_label.columns ## retrieving columns to group

x_train = training_data[columns] ## only getting the necesary columns
y_train = training_data['use [kW]']   ## dependent variables
## print(x_train)
x_train = np.reshape(x_train, (334, 10)) ## shaping the data


testing_data_without_label = np.reshape(testing_data_without_label, (31, 10)) ## shaping testing data

LR = LinearRegression() ## setting up linear regression model
LR = LR.fit(x_train,y_train) ## training the model

prediction =  LR.predict(testing_data_without_label) ## testing the model

## print(prediction)
## print(y_actual)

rms = sqrt(mean_squared_error(y_actual, prediction)) ## rms calculation

print("Root mean square error value : " + str(rms))

## creating csv file
dates = testing_data.index

with open('kw_prediction.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Date", "Predicted Value"])
    for i in range(len(prediction)):
        writer.writerow([dates[i], prediction[i]]) ## writing the data into the file
        
        

predictor = pd.read_csv('kw_prediction.csv')
print(predictor)

## Task 4
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score

weather_copy = grouped_merged_data.copy()
temperatures = weather_copy["temperature"]
binary_temp = []

## classfying high and low data
for i in range(len(temperatures)):
  if temperatures[i] < 35:
    binary_temp.append(0)
  else:
    binary_temp.append(1)

weather_copy["temperature"] = binary_temp ## adding column to the table

weather_copy.head(10)

training_data = weather_copy.head(334)
testing_data = weather_copy.tail(31)

y_actual = testing_data["temperature"]
testing_data_with_label = testing_data.copy()
del testing_data["temperature"]

columns = testing_data.columns
x_train = training_data[columns]   ## independent variables
y_train = training_data['temperature']   ## dependent variables

logreg = LogisticRegression() ## setting up logistic reg. model
logreg.fit(x_train,y_train) ## training log model

prediction = logreg.predict(testing_data) ## testing the model

f1score = f1_score(y_actual, prediction) ## calculating f1 score
print("Fl score: " + str(f1score))

dates = testing_data.index

## received a f1 score of around 0.7027

## creating csv file
with open('temp_predictor.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["Date", "Temperature Classification"])
    for i in range(len(prediction)):
        writer.writerow([dates[i], prediction[i]]) ## writing data into file
        
        

predictor = pd.read_csv('temp_predictor.csv')
print(predictor)

## Task 5
import matplotlib.pyplot as plt

energy_data2 = energy_data.copy()
date_and_time = energy_data2["Date & Time"]

just_time = []

## 6AM - 7PM is considered daytime, rest is night time

for i in range(len(date_and_time)):
  digit = int(date_and_time[i][11:13])
  if digit >= 6 and digit < 19:
    just_time.append(1) ## 1 is daytime
  else:
    just_time.append(0) ## 0 is nighttime

energy_data2["daytime"] = just_time

grouped_data_with_washer = energy_data2.groupby(["Date","daytime"])["Washer [kW]"].mean()
grouped_data_with_ac = energy_data2.groupby(["Date","daytime"])["AC [kW]"].mean()


y1_data = []
y2_data = []
x_data = []

print(grouped_data_with_washer)

for i in range(len(grouped_data_with_washer)):
  if i % 2 == 0:
    y1_data.append(grouped_data_with_washer[i]) ## y1 data contains nighttime data
    x_data.append(i / 2)
  else:
    y2_data.append(grouped_data_with_washer[i]) ##y2 data contains daytime data

## First graph is washer
plt.plot(x_data, y1_data, 'o', color='black'); ## black is nighttime
plt.plot(x_data, y2_data, 'o', color=''); ## red is daytime

plt.xlabel("Day (Jan 1 - Dec 31)")
plt.ylabel("Washer Usage per day")


## people generally use the washer more during the daytime rather than the night time

## Task 5 Continued

y1_data = []
y2_data = []
x_data = []

for i in range(len(grouped_data_with_ac)):
  if i % 2 == 0:
    y1_data.append(grouped_data_with_ac[i])
    x_data.append(i / 2)
  else:
    y2_data.append(grouped_data_with_ac[i])


## First graph is washer
plt.plot(x_data, y1_data, 'o',color='black')
plt.plot(x_data, y2_data, 'o',color='red')

plt.xlabel("Day (Jan 1 - Dec 31)")
plt.ylabel("A/C Usage per day")



## results show people use the ac more during the night time compared to the daytime and mostly during the summer months (Day 175 - Day 250) is roughly